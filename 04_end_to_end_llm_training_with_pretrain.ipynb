{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0",
      "metadata": {},
      "source": [
        "# End to end instruction example using LLMs with fine-tuning\n",
        "\n",
        "This notebook provides an example for how to use the pretrain data. This means that the model is trained on full patient histories, without any specific task. This can be used to develop models that can generate synthetic patients or embeddings.\n",
        "\n",
        "> **Note:** You need a GPU with at least 30GB of memory for this example to work.\n",
        "We also have not tested the performance of PEFT models - only as examples.\n",
        "\n",
        "> **Important:** Please install first the fine-tuning packages with `pip install twinweaver[fine-tuning-example]`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# nvidia-smi\n",
        "import os\n",
        "os.chdir(\"/data/gpfs/projects/punim2885/Xuan\")\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "import pandas as pd\n",
        "import gc\n",
        "import torch\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    pipeline,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "\n",
        "from twinweaver import (\n",
        "    DataManager,\n",
        "    Config,\n",
        "    ConverterPretrain\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Some key settings\n",
        "BASE_MODEL = \"microsoft/Phi-4-mini-instruct\"  # NOTE: we haven't tested the performance of this model beyond examples"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3",
      "metadata": {},
      "source": [
        "## Generate training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Input folder (after os.chdir to Xuan, paths are relative to that)\n",
        "INPUT_FOLDER = \"/data/gpfs/projects/punim2885/Xuan/Result/03_Demo_data\"\n",
        "\n",
        "# Load data\n",
        "df_events = pd.read_csv(f\"{INPUT_FOLDER}/events.csv\")\n",
        "df_constant = pd.read_csv(f\"{INPUT_FOLDER}/constants.csv\")\n",
        "df_constant_description = pd.read_csv(f\"{INPUT_FOLDER}/constants_description.csv\")\n",
        "\n",
        "# Ensure event_value is string (converter calls .lower() on it; floats/NaN cause AttributeError)\n",
        "if \"event_value\" in df_events.columns:\n",
        "    df_events[\"event_value\"] = df_events[\"event_value\"].fillna(\"\").astype(str).replace(\"nan\", \"\")\n",
        "\n",
        "# Use all constant columns except patientid\n",
        "config = Config()  # Override values here to customize pipeline\n",
        "config.constant_columns_to_use = [c for c in df_constant.columns if c != \"patientid\"]\n",
        "#config.constant_birthdate_column = \"birthyear\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:root:Found 30 out of 9083 missing dates in events \n"
          ]
        }
      ],
      "source": [
        "dm = DataManager(config=config)\n",
        "dm.load_indication_data(df_events=df_events, df_constant=df_constant, df_constant_description=df_constant_description)\n",
        "dm.process_indication_data()\n",
        "dm.setup_unique_mapping_of_events()\n",
        "dm.setup_dataset_splits()\n",
        "dm.infer_var_types()\n",
        "\n",
        "converter = ConverterPretrain(config=config, dm=dm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all training + validation patientids\n",
        "training_patientids = dm.get_all_patientids_in_split(config.train_split_name)\n",
        "validation_patientids = dm.get_all_patientids_in_split(config.validation_split_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9",
      "metadata": {},
      "source": [
        "The `generate_transformers_df` function iterates through each patient and generates the text data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "10",
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_transformers_df(patientids_list):\n",
        "    df = []\n",
        "\n",
        "    for patientid in patientids_list:\n",
        "        patient_data = dm.get_patient_data(patientid)\n",
        "\n",
        "        p_converted = converter.forward_conversion(\n",
        "            events=patient_data[\"events\"], \n",
        "            constant=patient_data[\"constant\"]\n",
        "        )\n",
        "        new_data = {\n",
        "            \"text\": p_converted[\"text\"],\n",
        "            \"patientid\": f\"{patientid}\",  # Just for ease of finding later\n",
        "        }\n",
        "        df.append(new_data)\n",
        "\n",
        "    df = pd.DataFrame(df)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "11",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate training and validation dfs\n",
        "df_train = generate_transformers_df(training_patientids)\n",
        "df_validation = generate_transformers_df(validation_patientids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "12",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>patientid</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The following is a patient, starting with the ...</td>\n",
              "      <td>VN010001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The following is a patient, starting with the ...</td>\n",
              "      <td>VN010003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The following is a patient, starting with the ...</td>\n",
              "      <td>VN010004</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The following is a patient, starting with the ...</td>\n",
              "      <td>VN010005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The following is a patient, starting with the ...</td>\n",
              "      <td>VN010006</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text patientid\n",
              "0  The following is a patient, starting with the ...  VN010001\n",
              "1  The following is a patient, starting with the ...  VN010003\n",
              "2  The following is a patient, starting with the ...  VN010004\n",
              "3  The following is a patient, starting with the ...  VN010005\n",
              "4  The following is a patient, starting with the ...  VN010006"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13",
      "metadata": {},
      "source": [
        "## Fine-tune LLM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14",
      "metadata": {},
      "source": [
        "We start by setting up the tokenizer. We set the padding token to be the same as the EOS (End of Sequence) token, which is a common practice for causal language models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "15",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup tokenizer and datasets\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "\n",
        "# Set padding token to eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "train_dataset = Dataset.from_pandas(df_train)\n",
        "validation_dataset = Dataset.from_pandas(df_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16",
      "metadata": {},
      "source": [
        "Instruction-tuned models expect data in a specific conversational format (e.g., User: ... Assistant: ...). \n",
        "We use `format_chat_template` to structure our raw prompt/completion strings into this list-of-messages format using the `user` and `assistant` roles.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "17",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1659429669c4bfc983d7bdab5841e37",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2256 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1da67744e1d4d9481e70a7a1e793cdb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/282 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Format data for chat template\n",
        "def format_chat_template(example):\n",
        "    \"\"\"Convert prompt/completion pairs to proper prompt/completion format\"\"\"\n",
        "    return {\n",
        "        \"text\": example[\"text\"],\n",
        "    }\n",
        "\n",
        "# Apply formatting to datasets\n",
        "train_dataset = train_dataset.map(format_chat_template)\n",
        "validation_dataset = validation_dataset.map(format_chat_template)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18",
      "metadata": {},
      "source": [
        "We configure 4-bit quantization using `BitsAndBytesConfig` (QLoRA). This significantly lowers memory usage, allowing us to fine-tune the model on consumer GPUs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "19",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define Quantization Config (4-bit loading)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,  # This should be set based on your GPU capabilities\n",
        "    bnb_4bit_use_double_quant=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20",
      "metadata": {},
      "source": [
        "Here we set up Low-Rank Adaptation (LoRA) configuration. `LoraConfig` defines the adapter parameters (rank `r`, `alpha`). we target linear layers (`q_proj`, `k_proj` etc.) which generally yields better results than just attending to query/value projections.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "21",
      "metadata": {},
      "outputs": [],
      "source": [
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=8,  # Rank (higher = more parameters to train)\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # Target all linear layers for best performance (specific to Llama architecture)\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22",
      "metadata": {},
      "source": [
        "We define the training arguments in `SFTConfig`. Notice the higher learning rate (`1e-4`) compared to typical full fine-tuning in the GDT paper. We also set `bf16=True` for newer GPUs (Ampere+) to improve training stability.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "23",
      "metadata": {},
      "outputs": [],
      "source": [
        "training_arguments = SFTConfig(\n",
        "    output_dir=\"./Result/04_Demo_results\",\n",
        "    #num_train_epochs=5,\n",
        "    num_train_epochs=2,\n",
        "    #per_device_train_batch_size=1,\n",
        "    per_device_train_batch_size=10,\n",
        "    gradient_accumulation_steps=1,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    save_steps=10,\n",
        "    logging_steps=10,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=10,\n",
        "    per_device_eval_batch_size=1,\n",
        "    learning_rate=1e-4,  # LR is higher for PEFT, see TwinWeaver paper for full fine-tuning details\n",
        "    fp16=False,  # Use fp16 for older GPUs T4/V100, bf16 for Ampere and later (A100/3090/4090)\n",
        "    bf16=True,\n",
        "    max_grad_norm=1.0,\n",
        "    warmup_ratio=0.1,\n",
        "    group_by_length=True,\n",
        "    save_total_limit=1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    #max_length=8192,\n",
        "    max_length=8192,\n",
        "    packing=False,  # Disable packing for more exact training, though can be activated\n",
        "    completion_only_loss=False,  # Compute loss on entire text\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "24",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "07cb14cfc7254d22b6f1cc6bf1ea4eb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "# Disable cache for training (required for gradient checkpointing)\n",
        "model.config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "25",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "610d92d6b44b4cf79c5969ae500a8c88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/2256 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f08b0f33be4843a497331312c2d63362",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/2256 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5a94e50b5b146c982daf42ff9d7015a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating train dataset:   0%|          | 0/2256 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "766a9aa024514f11abd830b8d42ffe75",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to eval dataset:   0%|          | 0/282 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb346547518a4263810f6a2c054543dc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing eval dataset:   0%|          | 0/282 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "993d00eaafc048fdb4e9c36e3b347430",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Truncating eval dataset:   0%|          | 0/282 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    processing_class=tokenizer,\n",
        "    args=training_arguments,\n",
        "    eval_dataset=validation_dataset,\n",
        "    peft_config=peft_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "26",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='226' max='226' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [226/226 12:06, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>4.416300</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>nan</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=226, training_loss=0.19541050058550538, metrics={'train_runtime': 730.665, 'train_samples_per_second': 6.175, 'train_steps_per_second': 0.309, 'total_flos': 1.44666853951488e+16, 'train_loss': 0.19541050058550538})"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Start training - takes around 5 mins, depending on hardware\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "27",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adapter saved to Result/04_Demo_results/final_adapter\n"
          ]
        }
      ],
      "source": [
        "# Save the fine-tuned adapter\n",
        "adapter_path = \"Result/04_Demo_results/final_adapter\"\n",
        "trainer.save_model(adapter_path)\n",
        "print(f\"Adapter saved to {adapter_path}\")\n",
        "\n",
        "del trainer\n",
        "del model\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28",
      "metadata": {},
      "source": [
        "## Inference example\n",
        "\n",
        "Inference example for a test set patient, where we want to generate the full patient trajectory after the first line of therapy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "29",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get the first test set patient\n",
        "test_patientid = dm.get_all_patientids_in_split(config.test_split_name)[0]\n",
        "patient_data = dm.get_patient_data(test_patientid)\n",
        "\n",
        "# Lets simulate forecasts for after the first line of therapy\n",
        "df_constant_patient = patient_data[\"constant\"].copy()\n",
        "df_events_patient = patient_data[\"events\"].copy()\n",
        "date_of_first_lot = df_events_patient.loc[\n",
        "    df_events_patient[\"event_category\"] == config.event_category_lot, \"date\"\n",
        "].min()\n",
        "date_of_first_event = df_events_patient[\"date\"].min()\n",
        "\n",
        "# Only keep data until (and including) first line of therapy\n",
        "df_events_patient = df_events_patient.loc[df_events_patient[\"date\"] <= date_of_first_lot]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31",
      "metadata": {},
      "source": [
        "We convert the patient data into the first part."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "32",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert to instruction\n",
        "converted = converter.forward_conversion(\n",
        "    events=df_events_patient, \n",
        "    constant=df_constant_patient\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33",
      "metadata": {},
      "source": [
        "For inference, we load the base model again (clean slate) to avoid any state from training, and then attach the adapter we trained. `PeftModel` handles the integration of the LoRA weights with the base model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34",
      "metadata": {},
      "source": [
        "For inference, we load the base model again (clean slate) and then attach the adapter we trained. `PeftModel` handles the integration of the LoRA weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "35",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdb739fc492e477593b7d2923f1063e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Phi3ForCausalLM(\n",
              "      (model): Phi3Model(\n",
              "        (embed_tokens): Embedding(200064, 3072, padding_idx=199999)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x Phi3DecoderLayer(\n",
              "            (self_attn): Phi3Attention(\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3072, out_features=3072, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3072, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (qkv_proj): Linear4bit(in_features=3072, out_features=5120, bias=False)\n",
              "            )\n",
              "            (mlp): Phi3MLP(\n",
              "              (gate_up_proj): Linear4bit(in_features=3072, out_features=16384, bias=False)\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=8192, out_features=3072, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.1, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=8192, out_features=8, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (activation_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "            (post_attention_layernorm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "            (resid_attn_dropout): Dropout(p=0.0, inplace=False)\n",
              "            (resid_mlp_dropout): Dropout(p=0.0, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (norm): Phi3RMSNorm((3072,), eps=1e-05)\n",
              "        (rotary_emb): Phi3RotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=3072, out_features=200064, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 1. Load the Base Model again (clean instance)\n",
        "adapter_path = \"Result/04_Demo_results/final_adapter\"\n",
        "base_model_inference = AutoModelForCausalLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    quantization_config=bnb_config,  # Reuse the 4-bit config\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=False,\n",
        ")\n",
        "\n",
        "# 2. Load the Saved Adapter\n",
        "# This wraps the base model with the fine-tuned LoRA layers\n",
        "inference_model = PeftModel.from_pretrained(base_model_inference, adapter_path)\n",
        "\n",
        "# 3. Switch to evaluation mode\n",
        "inference_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "36",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Create text generation pipeline\n",
        "# Re-enable cache for inference\n",
        "inference_model.config.use_cache = True\n",
        "text_gen_pipeline = pipeline(\"text-generation\", model=inference_model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The following is a patient, starting with the demographic data, following visit by visit everything that the patient experienced. All lab codes refer to LOINC codes.\n",
            "\n",
            "Starting with demographic data:\n",
            "\tYounger cohort=1; Older cohort=0 is Younger cohort,\n",
            "\tChild is in all rounds is yes,\n",
            "\tChild's sex is male,\n",
            "\tChild's first language is vietnamese,\n",
            "\tChild's ethnic group is Kinh,\n",
            "\tChild's religion is none.\n",
            "\n",
            "On the first visit, the patient experienced the following: \n",
            ".\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(converted[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "37",
      "metadata": {},
      "outputs": [],
      "source": [
        "# /data/gpfs/projects/punim2885/llm_dts/lib/python3.11/site-packages/twinweaver/common/config.py\n",
        "# Generate with LLM, for a given time\n",
        "generated_answer = text_gen_pipeline(\n",
        "    converted[\"text\"] + \"\\n\\n3 weeks later, the child visited and experienced the following:\",\n",
        "    #converted[\"text\"] ,\n",
        "    # max_new_tokens=128,\n",
        "    max_new_tokens=1024,\n",
        "    return_full_text=False,\n",
        "    do_sample=True,  # Using nucleus sampling\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        ")[0][\"generated_text\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "38",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " \n",
            "\tChild experienced fever with temperature of 38.4 degrees Celsius\n",
            "\tChild experienced headache with severity of 4\n",
            "\tChild experienced sore throat with severity of 3\n",
            "\tChild experienced cough with severity of 3\n",
            "\tChild experienced nasal congestion with severity of 2\n",
            "\tChild experienced facial pain with severity of 2\n",
            "\tChild experienced pharyngeal erythema with severity of 2\n",
            "\tChild experienced ear pain with severity of 2\n",
            "\tChild experienced difficulty swallowing with severity of 3\n",
            "\tChild experienced nasal discharge with color yellow and consistency thick\n",
            "\tChild experienced nausea with severity of 3\n",
            "\tChild experienced vomiting with frequency of 2 and duration of 5 minutes\n",
            "\tChild experienced nasal blockage with nasal obstruction score of 2\n",
            "\tChild experienced nasal blockage with nasal congestion score of 2\n",
            "\tChild experienced nasal blockage with nasal discharge score of 2\n",
            "\tChild experienced nasal blockage with facial pain score of 2\n",
            "\tChild experienced nasal blockage with pharyngeal erythema score of 2\n",
            "\tChild experienced nasal blockage with ear pain score of 2\n",
            "\tChild experienced nasal blockage with difficulty swallowing score of 3\n",
            "\tChild experienced nasal blockage with nasal discharge score of 2\n",
            "\tChild experienced nasal blockage with nasal congestion score of 2\n",
            "\tChild experienced nasal blockage with facial pain score of 2\n",
            "\tChild experienced nasal blockage with pharyngeal erythema score of 2\n",
            "\tChild experienced nasal blockage with ear pain score of 2\n",
            "\tChild experienced nasal blockage with difficulty swallowing score of 3\n",
            "\tChild experienced nasal blockage with nasal discharge score of 2\n",
            "\tChild experienced nasal blockage with nasal congestion score of 2\n",
            "\tChild experienced nasal blockage with facial pain score of 2\n",
            "\tChild experienced nasal blockage with pharyngeal erythema score of 2\n",
            "\tChild experienced nasal blockage with ear pain score of 2\n",
            "\tChild experienced nasal blockage with difficulty swallowing score of 3\n",
            "\tChild experienced nasal blockage with nasal discharge score of 2\n",
            "\tChild experienced nasal blockage with nasal congestion score of 2\n",
            "\tChild experienced nasal blockage with facial pain score of 2\n",
            "\tChild experienced nasal blockage with pharyngeal erythema score of 2\n",
            "\tChild experienced nasal blockage with ear pain score of 2\n",
            "\tChild experienced nasal blockage with difficulty swallowing score of 3\n",
            "\tChild experienced nasal blockage with nasal discharge score of 2\n",
            "\tChild experienced nasal blockage with nasal congestion score of 2\n",
            "\tChild experienced nasal blockage with facial pain score of 2\n",
            "\tChild experienced nasal blockage with pharyngeal erythema score of 2\n",
            "\tChild experienced nasal blockage with ear pain score of 2\n",
            "\tChild experienced nasal blockage with difficulty swallowing score of 3\n",
            "\tChild experienced nasal blockage with nasal discharge score of 2\n",
            "\tChild experienced nasal blockage with nasal congestion score of 2\n",
            "\tChild experienced nasal blockage with facial pain score of 2\n",
            "\tChild experienced nasal blockage with pharyngeal erythema score of 2\n",
            "\tChild experienced nasal blockage with ear pain score of 2\n",
            "\tChild experienced nasal blockage with difficulty swallowing score of 3\n",
            "\tChild experienced nasal blockage with nasal discharge score of 2\n",
            "\tChild experienced nasal blockage with nasal congestion score of 2\n",
            "\tChild experienced nasal blockage with facial pain score of 2\n",
            "\tChild experienced nasal blockage with pharyngeal erythema score of 2\n",
            "\tChild experienced nasal blockage with ear pain score of 2\n",
            "\tChild experienced nasal blockage with difficulty swallowing score of 3\n",
            "\tChild experienced nasal blockage with nasal discharge score of 2\n",
            "\tChild experienced nasal blockage with nasal congestion score of 2\n",
            "\tChild experienced nasal blockage with facial pain score of 2\n",
            "\tChild experienced nasal blockage with pharyngeal erythema score of 2\n",
            "\tChild experienced nasal blockage with ear pain score of 2\n",
            "\tChild experienced nasal blockage with difficulty swallowing score of 3\n",
            "\tChild experienced nasal blockage with nasal discharge score of 2\n",
            "\tChild experienced nasal blockage with nasal congestion score of 2\n",
            "\tChild experienced nasal blockage with facial pain score of 2\n",
            "\tChild experienced nasal blockage with pharyngeal erythema score of 2\n",
            "\tChild experienced nasal blockage with ear pain score of 2\n",
            "\tChild experienced nasal blockage with difficulty swallowing score of 3\n",
            "\tChild experienced nasal blockage with nasal discharge score of 2\n",
            "\tChild experienced nasal blockage with nasal congestion score of 2\n",
            "\tChild experienced nasal blockage with facial pain score of 2\n",
            "\tChild experienced nasal blockage with pharyngeal erythema score of 2\n",
            "\tChild experienced nasal blockage with ear pain score of 2\n",
            "\tChild experienced nasal blockage with difficulty swallowing score of 3\n",
            "\tChild experienced nasal blockage with nasal discharge score of 2\n",
            "\tChild experienced nasal blockage with nasal congestion score of 2\n",
            "\tChild experienced nasal blockage with facial pain score of \n"
          ]
        }
      ],
      "source": [
        "# Show the generated answer\n",
        "print(generated_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39",
      "metadata": {},
      "source": [
        "The raw text output from the model needs to be parsed back into structured data. `reverse_conversion` handles this, returning a dictionary with the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "40",
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "'DataManager' object is not subscriptable",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[57]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Reverse convert\u001b[39;00m\n\u001b[32m      2\u001b[39m full_trajectory = converted[\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m] + generated_answer\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m ret_dict = \u001b[43mconverter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreverse_conversion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_trajectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate_of_first_event\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/data/gpfs/projects/punim2885/Xuan/env/llm_dts/lib/python3.11/site-packages/twinweaver/pretrain/converter_pretrain.py:154\u001b[39m, in \u001b[36mConverterPretrain.reverse_conversion\u001b[39m\u001b[34m(self, text, meta_data, unique_events)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[33;03mConverts a textual representation of patient data back into structured DataFrames.\u001b[39;00m\n\u001b[32m    125\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    151\u001b[39m \u001b[33;03m    }\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# Extract constant data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m constant_data = \u001b[38;5;28mself\u001b[39m._extract_constant_data(text, \u001b[43mmeta_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mconstant_description\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# Extract event data\u001b[39;00m\n\u001b[32m    157\u001b[39m event_data = \u001b[38;5;28mself\u001b[39m._extract_event_data(text, unique_events, meta_data[\u001b[33m\"\u001b[39m\u001b[33mraw_events\u001b[39m\u001b[33m\"\u001b[39m])\n",
            "\u001b[31mTypeError\u001b[39m: 'DataManager' object is not subscriptable"
          ]
        }
      ],
      "source": [
        "# Reverse convert\n",
        "full_trajectory = converted[\"text\"] + generated_answer\n",
        "ret_dict = converter.reverse_conversion(full_trajectory, dm, date_of_first_event)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "41",
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'ret_dict' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mret_dict\u001b[49m[\u001b[33m\"\u001b[39m\u001b[33mevents\u001b[39m\u001b[33m\"\u001b[39m].head()\n",
            "\u001b[31mNameError\u001b[39m: name 'ret_dict' is not defined"
          ]
        }
      ],
      "source": [
        "ret_dict[\"events\"].head()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "llm_dts",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
